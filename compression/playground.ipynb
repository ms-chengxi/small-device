{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmikaelyan/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigname=\"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(bigname,use_fast=False)\n",
    "assert not tokenizer.legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "bigname=\"microsoft/Phi-3.5-mini-instruct\"\n",
    "bigmodel = AutoModelForCausalLM.from_pretrained(bigname,  device_map=\"cpu\",torch_dtype=torch.float16,\n",
    "                                                trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.2.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.2.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.2.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([3072])\n",
      "total A 73.3127109662652\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrices = []\n",
    "\n",
    "for name, param in bigmodel.named_parameters():\n",
    "    if \".1.\" in name and \"o_proj\" in name :\n",
    "        layer_5_oproj = param.detach().cpu().numpy()\n",
    "        print(name)\n",
    "    if \".2.\" in name and \"o_proj\" in name :\n",
    "        layer_6_oproj = param.detach().cpu().numpy()\n",
    "        print(name)\n",
    "    if \".2.\" in name:\n",
    "        print(name, param.shape)\n",
    "    if (\".1.\" in name or \".2.\" in name or \".3.\" in name or \".4.\" in name or \".5.\" in name) and \"o_proj\" in name:\n",
    "        matrices.append(param.detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "A = np.array(layer_5_oproj, dtype=np.float64, copy=True)\n",
    "B = np.array(layer_6_oproj, dtype=np.float64, copy=True)\n",
    "print('total A', np.linalg.norm(A, 'fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 3072)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have two weight matrices, A and B, each of shape (3072, 3072)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_layers():\n",
    "    layer_all_o_proj = []\n",
    "    for name, param in bigmodel.named_parameters():\n",
    "        if \"o_proj\" in name:\n",
    "            layer_all_o_proj.append(param.detach().cpu().numpy())\n",
    "\n",
    "    return layer_all_o_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.linalg\n",
    "\n",
    "def orthogonal_procrustes(data1, data2, check_finite=True):\n",
    "    if check_finite:\n",
    "        data1 = np.asarray_chkfinite(data1)\n",
    "        data2 = np.asarray_chkfinite(data2)\n",
    "    else:\n",
    "        data1 = np.asanyarray(data1)\n",
    "        data2 = np.asanyarray(data2)\n",
    "    if data1.ndim != 2:\n",
    "        raise ValueError('expected ndim to be 2, but observed %s' % data1.ndim)\n",
    "    if data1.shape != data2.shape:\n",
    "        raise ValueError(f'the shapes of A and B differ ({data1.shape} vs {data2.shape})')\n",
    "    # Be clever with transposes, with the intention to save memory.\n",
    "    u, w, vt = scipy.linalg.svd(data2.T.dot(data1).T)\n",
    "    R = u.dot(vt)\n",
    "    R_ = u[:, :].dot(vt[:, :])\n",
    "    scale = w.sum()\n",
    "\n",
    "    # n = u[:, 0]\n",
    "    # # Ensure that n is a unit vector\n",
    "    # n = n / np.linalg.norm(n)\n",
    "    # R = R_ = np.eye(data2.shape[1]) - 2 * np.outer(n ,n)\n",
    "    # scale = 1\n",
    "    # R = np.array(R, dtype=np.float64, copy=True)\n",
    "    # u, w, vt = scipy.linalg.svd(R)\n",
    "    # R = u[:, :256] @ np.diag(w[:256]) @ vt[:256, :]\n",
    "    return R, scale, R_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes(data1, data2):\n",
    "    mtx1 = np.array(data1, dtype=np.float64, copy=True)\n",
    "    mtx2 = np.array(data2, dtype=np.float64, copy=True)\n",
    "\n",
    "    if mtx1.ndim != 2 or mtx2.ndim != 2:\n",
    "        raise ValueError(\"Input matrices must be two-dimensional\")\n",
    "    if mtx1.shape != mtx2.shape:\n",
    "        raise ValueError(\"Input matrices must be of same shape\")\n",
    "    if mtx1.size == 0:\n",
    "        raise ValueError(\"Input matrices must be >0 rows and >0 cols\")\n",
    "\n",
    "    # translate all the data to the origin\n",
    "    mean_1 = np.mean(mtx1, 0)\n",
    "    mean_2 = np.mean(mtx2, 0)\n",
    "    mtx1 -= mean_1\n",
    "    mtx2 -= mean_2\n",
    "\n",
    "    norm1 = np.linalg.norm(mtx1)\n",
    "    norm2 = np.linalg.norm(mtx2)\n",
    "\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        raise ValueError(\"Input matrices must contain >1 unique points\")\n",
    "\n",
    "    # change scaling of data (in rows) such that trace(mtx*mtx') = 1\n",
    "    mtx1 /= norm1\n",
    "    mtx2 /= norm2\n",
    "    mtx2_scaled = mtx2.copy()\n",
    "    # transform mtx2 to minimize disparity\n",
    "    # R, s = scipy.linalg.orthogonal_procrustes(mtx1, mtx2)\n",
    "    R, s, R_ = orthogonal_procrustes(mtx1, mtx2)\n",
    "    \n",
    "    mtx2 = np.dot(mtx2, R_.T) * s\n",
    "\n",
    "    # measure the dissimilarity between the two datasets\n",
    "    disparity = np.sum(np.square(mtx1 - mtx2))\n",
    "\n",
    "    return mtx1, mtx2, mtx2_scaled, R_, s, mean_1, mean_2, norm1, norm2, disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 3072)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mtx1, mtx2, mtx2_scaled, R_, s, mean_1, mean_2, norm_1, norm_2, disparity = procrustes(layer_5_oproj, layer_6_oproj)\n",
    "A_scaled = mtx1\n",
    "B_scaled = mtx2_scaled\n",
    "mtx2_estimate = np.dot(B_scaled, R_.T) * s\n",
    "print(R_.shape)\n",
    "# Delta = mtx1 - mtx2\n",
    "# plt.plot(mtx1 - mtx2)\n",
    "# plt.xlabel('Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('Plot of delta 5 and 6 after procrustes')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "delta = mtx1 - mtx2\n",
    "orig_delta = A-B\n",
    "scaled_delta = mtx1 - mtx2_scaled\n",
    "\n",
    "Ud, Sd, VTd = scipy.linalg.svd(delta)\n",
    "U, S, VT = scipy.linalg.svd(A)\n",
    "# Ud2, Sd2, VTd2 = scipy.linalg.svd(orig_delta)\n",
    "# Ud3, Sd3, VTd3 = scipy.linalg.svd(scaled_delta)\n",
    "# Us, Ss, VTs = scipy.linalg.svd(A_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total A 73.3127109662652\n",
      "Reconstruction error original: 30.734716702842555\n",
      "Reconstruction error rotated: 19.235566208154548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(19.235566208154548)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_approx_matrix(U, S, VT, k=128):\n",
    "    U_k = U[:, :k]\n",
    "    S_k = np.diag(S[:k])\n",
    "    VT_k = VT[:k, :]\n",
    "\n",
    "    return U_k @ S_k @ VT_k\n",
    "\n",
    "def approx_error_calc_original(A, U, S, VT, k=128):\n",
    "\n",
    "    A_approx = get_approx_matrix(U, S, VT, k)\n",
    "    reconstruction_error = np.linalg.norm(A - A_approx, 'fro')\n",
    "    # reconstruction_error = np.sum(np.abs(A - A_approx))\n",
    "    print(\"Reconstruction error original:\", reconstruction_error)\n",
    "    return reconstruction_error\n",
    "\n",
    "def approx_error_calc_original_scaled(A, U, S, VT, k=128):\n",
    "\n",
    "    A_approx_scaled = get_approx_matrix(U, S, VT, k)\n",
    "    A_approx = A_approx_scaled * norm_1\n",
    "    A_approx += mean_1\n",
    "    reconstruction_error = np.linalg.norm(A - A_approx, 'fro')\n",
    "    # reconstruction_error = np.sum(np.abs(A - A_approx))\n",
    "    print(\"Reconstruction error original scaled:\", reconstruction_error)\n",
    "    return reconstruction_error\n",
    "\n",
    "def approx_error_calc_original_delta(A, B, U, S, VT, k=128):\n",
    "\n",
    "    delta_approx = get_approx_matrix(U, S, VT, k)\n",
    "    A_approx = delta_approx + B\n",
    "    reconstruction_error = np.linalg.norm(A - A_approx, 'fro')\n",
    "    # reconstruction_error = np.sum(np.abs(A - A_approx))\n",
    "    print(\"Reconstruction error original delta:\", reconstruction_error)\n",
    "    return reconstruction_error\n",
    "\n",
    "def approx_error_calc_scaled_delta(A, B_scaled, U, S, VT, k=128):\n",
    "\n",
    "    scaled_delta_approx = get_approx_matrix(U, S, VT, k)\n",
    "    A_scaled_approx = scaled_delta_approx + B_scaled\n",
    "    A_approx = A_scaled_approx * norm_1\n",
    "    A_approx += mean_1\n",
    "    # print(mean_1)\n",
    "    reconstruction_error = np.linalg.norm(A - A_approx, 'fro')\n",
    "    # reconstruction_error = np.sum(np.abs(A - A_approx))\n",
    "    print(\"Reconstruction error scaled delta:\", reconstruction_error)\n",
    "    return reconstruction_error\n",
    "\n",
    "def approx_error_calc_rotated(A, B_rotated, U, S, VT, k=128):\n",
    "\n",
    "    rotated_delta_approx = get_approx_matrix(U, S, VT, k)\n",
    "    A_scaled_approx = rotated_delta_approx + B_rotated\n",
    "    A_approx = A_scaled_approx * norm_1\n",
    "    A_approx += mean_1\n",
    "    # print(mean_1)\n",
    "    reconstruction_error = np.linalg.norm(A - A_approx, 'fro')\n",
    "    # reconstruction_error = np.sum(np.abs(A - A_approx))\n",
    "    print(\"Reconstruction error rotated:\", reconstruction_error)\n",
    "    return reconstruction_error\n",
    "# print('delta')\n",
    "# approx_error_calc(delta, Ud, Sd, VTd, k=128)\n",
    "# print('original')\n",
    "# approx_error_calc(mtx1, U, S, VT, k=128)\n",
    "# print('original delta')\n",
    "\n",
    "print('total A', np.linalg.norm(A, 'fro'))\n",
    "# print('total A', np.sum(np.abs(A)))\n",
    "approx_error_calc_original(A, U, S, VT, k=1000)\n",
    "# approx_error_calc_original_scaled(A, Us, Ss, VTs, k=1024)\n",
    "# approx_error_calc_original_delta(A, B, Ud2, Sd2, VTd2, k=1024)\n",
    "\n",
    "# approx_error_calc_scaled_delta(A, B_scaled, Ud3, Sd3, VTd3, k=1024)\n",
    "approx_error_calc_rotated(A, mtx2_estimate, Ud, Sd, VTd, k=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task_vector2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

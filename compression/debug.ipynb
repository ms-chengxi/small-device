{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name=\"microsoft/Phi-3.5-mini-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "total A 81.69725469101107\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \".2.\" in name and \"o_proj\" in name :\n",
    "        layer_1_oproj = param.detach().cpu().numpy()\n",
    "        print(name)\n",
    "    if \".3.\" in name and \"o_proj\" in name :\n",
    "        layer_2_oproj = param.detach().cpu().numpy()\n",
    "        print(name)\n",
    "        \n",
    "A = np.array(layer_1_oproj, dtype=np.float64, copy=True)\n",
    "B = np.array(layer_2_oproj, dtype=np.float64, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_layers():\n",
    "    pairs = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have two weight matrices, A and B, each of shape (3072, 3072)\n",
    "# orthogonal procrustes problem: find an orthogonal matrix Q which most closely maps A to B. \n",
    "# Q is the matrix that minimizes ||QA - B|| subject to Q^TQ = I \n",
    "# solution: Q = UV^T where UV^T is the SVD of BA^T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.linalg\n",
    "\n",
    "def orthogonal_procrustes(A, B):\n",
    "    # Be clever with transposes, with the intention to save memory.\n",
    "    u, w, vt = scipy.linalg.svd(B.T.dot(A).T)\n",
    "    R = u.dot(vt)\n",
    "    scale = w.sum()\n",
    "    return R, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes(data1, data2):\n",
    "    mtx1 = np.array(data1, dtype=np.float64, copy=True)\n",
    "    mtx2 = np.array(data2, dtype=np.float64, copy=True)\n",
    "\n",
    "    if mtx1.ndim != 2 or mtx2.ndim != 2:\n",
    "        raise ValueError(\"Input matrices must be two-dimensional\")\n",
    "    if mtx1.shape != mtx2.shape:\n",
    "        raise ValueError(\"Input matrices must be of same shape\")\n",
    "    if mtx1.size == 0:\n",
    "        raise ValueError(\"Input matrices must be >0 rows and >0 cols\")\n",
    "\n",
    "    # translate all the data to the origin\n",
    "    mean_1 = np.mean(mtx1, 0)\n",
    "    mean_2 = np.mean(mtx2, 0)\n",
    "    mtx1 -= mean_1\n",
    "    mtx2 -= mean_2\n",
    "\n",
    "    norm1 = np.linalg.norm(mtx1)\n",
    "    norm2 = np.linalg.norm(mtx2)\n",
    "\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        raise ValueError(\"Input matrices must contain >1 unique points\")\n",
    "\n",
    "    # change scaling of data (in rows) such that trace(mtx*mtx') = 1\n",
    "    mtx1 /= norm1\n",
    "    mtx2 /= norm2\n",
    "    mtx2_scaled = mtx2.copy()\n",
    "    # transform mtx2 to minimize disparity\n",
    "    # R, s = scipy.linalg.orthogonal_procrustes(mtx1, mtx2)\n",
    "    R, s = orthogonal_procrustes(mtx1, mtx2)\n",
    "    \n",
    "    mtx2 = np.dot(mtx2, R.T) * s\n",
    "\n",
    "    # measure the dissimilarity between the two datasets\n",
    "    disparity = np.sum(np.square(mtx1 - mtx2))\n",
    "\n",
    "    return mtx1, mtx2, mtx2_scaled, R, s, mean_1, mean_2, norm1, norm2, disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_final, B_final, B_scaled, R, scale, mean_1, mean_2, norm_1, norm_2, disparity = procrustes(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = A_final - B_final\n",
    "orig_delta = A - B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_rank_matrix(U, S, VT, rank=128):\n",
    "    U_k = U[:, :rank]\n",
    "    S_k = np.diag(S[:rank])\n",
    "    VT_k = VT[:rank, :]\n",
    "\n",
    "    return U_k @ S_k @ VT_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ud, Sd, VTd = scipy.linalg.svd(delta)\n",
    "U, S, VT = scipy.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rank_delta = get_low_rank_matrix(Ud, Sd, VTd, rank=3072)\n",
    "low_rank_A = get_low_rank_matrix(U, S, VT, rank=3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_error_calc_original(matrix, k=128):\n",
    "    U, S, VT = scipy.linalg.svd(matrix)\n",
    "    matrix_approx = get_low_rank_matrix(U, S, VT, k)\n",
    "    reconstruction_error = np.linalg.norm(matrix - matrix_approx, 'fro')\n",
    "    return reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.9086753689514116e-13)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_error_calc_original(A, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_error_calc_rotated(A, B_rotated, delta, k=128):\n",
    "    Ud, Sd, VTd = scipy.linalg.svd(delta)\n",
    "    rotated_delta_approx = get_low_rank_matrix(Ud, Sd, VTd, k)\n",
    "    A_scaled_approx = rotated_delta_approx + B_rotated\n",
    "    A_approx = A_scaled_approx * norm_1\n",
    "    A_approx += mean_1\n",
    "    reconstruction_error = np.linalg.norm(A - A_approx, 'fro')\n",
    "    return reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.4065190971944947e-13)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_rotated = np.dot(B_scaled, R.T) * scale\n",
    "approx_error_calc_rotated(A, B_rotated, delta, k=3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reconstruction_error():\n",
    "    # with the same rank, the error should be zero\n",
    "    error_orig = approx_error_calc_original(A, 3072)\n",
    "    error_rotated = approx_error_calc_rotated(A, B_rotated, delta, 3072)\n",
    "    np.testing.assert_allclose(error_orig, 0, atol=1e-12)\n",
    "    np.testing.assert_allclose(error_rotated, 0, atol=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reconstruction_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
